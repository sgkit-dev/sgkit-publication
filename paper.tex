%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% ELIFE ARTICLE TEMPLATE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% PREAMBLE
\documentclass[9pt,lineno]{elife}
% Use the onehalfspacing option for 1.5 line spacing
% Use the doublespacing option for 2.0 line spacing
% Please note that these options may affect formatting.
% Additionally, the use of the \newcommand function should be limited.

% Systematise how we typeset elements of the API (functions, variables, etc)
\newcommand{\sgapi}[1]{\texttt{#1}}
\newcommand{\toolname}[1]{\texttt{#1}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% ARTICLE SETUP
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{Scalable statistical genetic data analysis in Python}

\author[1]{Eric Czech}
\author[1,2\authfn{1}\authfn{3}]{Firstname Middlename Familyname}
\author[2\authfn{1}\authfn{4}]{Firstname Initials Surname}
\author[2*]{Firstname Surname}
\affil[1]{Related Sciences LLC}
\affil[2]{Institution 2}

\corr{email1@example.com}{FMS}
\corr{email2@example.com}{FS}

\contrib[\authfn{1}]{These authors contributed equally to this work}
\contrib[\authfn{2}]{These authors also contributed equally to this work}

\presentadd[\authfn{3}]{Department, Institute, Country}
\presentadd[\authfn{4}]{Department, Institute, Country}
% \presentadd[\authfn{5}]{eLife Sciences editorial Office, eLife Sciences, Cambridge, United Kingdom}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% ARTICLE START
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\maketitle

% Please provide an abstract of no more than 150 words. Your abstract should
% explain the main contributions of your article, and should not contain any
% material that is not included in the main text.

\begin{abstract}
The PyData ecosystem is an umbrella term covering Python packages based on a
broad range of modern techniques, such as chunk-compressed columnar data
storage, just-in-time compilation of numerical code, and scaling of
calculations across clusters of computers. Together, these technologies have
been successfully applied in scientific applications using data at the petabyte
scale. These technologies, and the many benefits that they provide, however,
have not been successfully applied in the field of genomics, which is currently
making the transition to working at petabyte scale. We present sgkit, a Python
package designed to bring the benefits of the PyData ecosystem to genomics,
allowing users to efficiently analyse large-scale data using familiar tools. We
discuss the underlying design principles of these technologies and illustrate
their suitability in genetics and genomics applications, via examples on
large-scale datasets such as UK Biobank.
\end{abstract}

\section{Introduction}

% This current phrasing would annoy people, just spewing this
% out to roughly say what I think is relevant, we can refine later.
The study of genetics is currently transforming into a true big-data science.
Since the Human Genome Project, genomics has required working with
signicant volumes of data and the specialised methods
required to transform the raw biological signals into analysable data,
such as assembly[cite], mapping[cite] and variant calling[cite] are compute intensive.
However, beyond these initial steps required to generate the data, the actual
analysis of the genetic variation could typically be performed on a single
computer, and often with ad-hoc software written using shell utilities
and scripting languages such as Perl.
From the Human Genome Project, through 1000 Genomes, and now with
population scale genomics datasets such as UK Biobank, GeL, etc,
the requirements for analysing the data have far outgrown these
methods. Simple queries can require minutes to complete, which
has a negative effect on researcher productivity, as well as limiting
the analyses that can be performed. The financial cost of working
with such datasets is also substantial, which limits accessibility
to an elite set of well-funded groups.

[Summary of what we do and paper roadmap]

\section{Results}

\subsection{Sgkit design principles}
Openness is a core organising principle of sgkit, both in terms of
development processes and code licensing, and also with respect
to usability. By following the general philosophy of array-oriented
computing, the functionality in sgkit is organised around the
key concept of a ``dataset`` which encapsulates a set of
related ``variables''. The implementation aims to be as agnostic
as possible as to the underlying implementation of these arrays,
which can be stored in-memory as NumPy arrays, in chunked
compressed on-disk storage as Zarr arrays, or ...

[In later sections we discuss enabling technologies in detail,
here we talk about how we use them from a high-level, and layout
the overall design philosophy. This isn't documentation, rather
it's the forum where we lay out explicitly early design choices
and their rationale.]

\subsection{Statistical Genetics}
Statistical genetics is largely concerned with disease associations
in humans, and the success of genome wide association studies (GWAS) has led
to explosive growth in the field. Sample size is the single most
important factor in driving statistical power, and
genotype array data has been collected for millions of samples [REFS].
Because of the relative heterogenity of the data sources and
required tasks, programs like \toolname{plink} are able to provide
an end-to-end analysis pipline from the source genotype data, quality
control and associations via Manhattan plots.

Early studies ecompassed X samples, but now we have UK Biobank
and large meta analyses [GIANT height paper]. Single computer platforms
like plink are no longer sufficient, so we need things like Hail.
Hail has enabled a whole bunch of stuff. Recently we have been moving
Whole Exome [UKB whole exome paper]
and even WGS data [UKB whole genome paper] for hundreds of thousands
of samples, raising the volumes of data to be processed even higher.

[TODO]

\subsection{Population Genetics}
[Things to work in here:
1) Classically popgen is concerned with allele frequencies, and so
used a variety of different data types to access this.
2) Data was at a pretty small scale, so you could do the entire
analysis within a single monolithic package like Arlequin.
3) people have been transitioning to WGS data recently.
]

Population geneticists study evolution at the population scale,
characterising the evolutionary forces shaping the diversity
within and between populations. Compared to statistical genetics,
which is largely concerned with humans, population geneticists
study a broad range of organisms across the tree of life, both well
characterised ``model'' species, and those for which the most basic
genetic information (such as the sizes and number of chromosomes,
and ploidy level) must be determined.
Sample sizes are also much smaller,
with dozens to a few hundred genomes per species being typical.
However, projects such as Darwin Tree of
Life~\citep{darwin2022sequence} are currently sequencing tens of
thousands of different species.

% JK: I'm making this up - need to get something concrete to back it up.
% Is there any review paper summarising computation in popgen?
[THIS IS A ROUGH DRAFT: I'm just getting in a paragraph here with citations
to tools/packages that are important in PopGen. We can get a narrative
through it later.]
Because of the relatively small volumes of data and the variety
of different organisms, statistical analysis of population genetic
data has tended to be done by custom scripts. Tools such
as \toolname{plink}~\citep{purcell2007plink}
are often not applicable because of the built-in
assumptions that are appropriate only for humans and other model organisms.
Programs such as Arlequin~\citep{excoffier2005arlequin,excoffier2010arlequin}
contain a large suite of features, but can only be accessed by GUI
or command line interfaces.
BioPerl~\citep{stajich2002bioperl} and BioPython~\citep{cock2009biopython}
are more concerned with alignments and providing access to command-line
programs, and are oriented towards flexibility rather than
performance.
The \toolname{scikit-allel}~\citep{miles2023scikit}
and \toolname{pylibseq}~\citep{thornton2003libsequence}
libraries provide efficient programmatic
access to a rich suite of population genetic statistics, but have limited
support for parallelisation.

% Discussion of simulated data and tree-based stuff.
Population geneticists are often interested in comparing with simulated data.
Until recently, this was done by processing
custom text formats output by programs such as
\toolname{ms}~\citep{hudson2002generating} and accompanying
scripts to compute basic statistics.
% JK: This is a bit of a self-cite fest here, but I think people would
% wonder what the connection between sgkit and tskit is if we don't
% explain it. Happy to discuss if anyone doesn't like it, though.
The \toolname{msprime}~\citep{kelleher2016efficient} simulator took a
different approach by providing a Python API which gives
access to the underlying genealogical trees, which often allow
statistics to be calculated more efficiently than standard
matrix-based approaches~\citep{ralph2020efficiently}.
These methods have been subsequently generalised to
other forms of
simulation~\citep{kelleher2018efficient,haller2018tree}
as well as being inferred from observed variation
data~\citep{kelleher2019inferring,wohns2022unified},
supported by the open-source \toolname{tskit}
library~\citep{tskit2023tskit}.
See~\cite{baumdicker2021efficient} for further discussion on
computation in population genetic simulations, and the
\texttt{tskit} software ecosystem.
% Need to say this somewhere, or people will wonder. This is a
% rough draft
While tree-based methods are excellent for simulated data
and attractive for real-data analysis, they are not a solution
to all problems. Firstly, not all datasets are appropriate
for tree inference and secondly, before we infer trees we must
first perform extensive quality control of the raw data, necessarily
in variant-matrix form.

\subsubsection{API overview}

Sgkit provides a number of methods for computing statistics in population
genetics. All methods work on an Xarray dataset that has a number of well-known
variables defined [TODO: see section...], which for population genetics are the
genotype calls, stored in the \sgapi{call\_genotype} variable. The methods return a
new dataset containing variables that store the computed statistics.

Before running the methods, the dataset is usually divided into windows along
the genome, using the \sgapi{window\_by} functions, which tell sgkit to produce
per-window statistics. For example, \sgapi{window\_by\_position} creates windows that
are a fixed number of base pairs, while \sgapi{window\_by\_interval} creates windows
corresponding to arbitrary user-defined intervals.

It's common in population genetics to group samples into populations, which in
sgkit are referred to as \emph{cohorts}. There are two types of statistics: one-way
statistics where there is a single statistic for each cohort, and multi-way
statistics where there is a statistic between each pair, triple, etc of
cohorts. [TODO: do we need to say how cohorts are defined?]

The methods for one-way statistics include \sgapi{diversity} for computing mean
genetic diversity, \sgapi{Tajimas\_D} for computing Tajimaâ€™s D, and
\sgapi{Garud\_H} for
computing the H1, H12, H123 and H2/H1 statistics~\citep{garud2015recent}.

The methods for multi-way statistics include \sgapi{divergence} and
\sgapi{Fst} for
computing mean genetic divergence and $F_{ST}$ (respectively) between pairs of
cohorts, and \sgapi{pbs} for computing the population branching statistic between
cohort triples.

\subsubsection{Example}

We converted phased Ag1000G hypotype data in Zarr format
%[@https://www.malariagen.net/data/ag1000g-phase-2-ar1]
to sgkit's Zarr format
using the \sgapi{read\_scikit\_allel\_vcfzarr} function.
The data contained 1,164
samples at 39,604,636 sites, and was [TODO] MB on disk before conversion, and
[TODO] MB after conversion to sgkit's Zarr format. Data for the X chromosome
was discarded since it was not available for all samples. The conversion took
[TODO: X minutes Y seconds], including a postprocessing ``rechunk'' step to
ensure that the data was suitably chunked for the subsequent analysis.

We grouped samples into 15 cohorts based on location using simple Pandas and
Xarray data manipulation functions. We then set up half-overlapping windows
along the genome using sgkit's \sgapi{window\_by\_position} function with a fixed
window size of 6000 base pairs, and a step size of 3000 base pairs.

We computed H statistics for one cohort using the \sgapi{Garud\_H} function in sgkit.
This operation took [TODO: X minutes Y seconds]. Finally, we verified that the
output was concordant with scikit-allel. [TODO: how hard to reproduce the
scikit-allel visualization?]

[TODO: summarize the biologically interesting thing here]

\subsection{Quantitative Genetics}

[TODO]


\subsection{Enabling technologies}

[High-level summary of where things are in terms of data analysis
in genomics, pointing forwards to
the individual use-cases of popgen, statgen etc to discuss the
individual tools.]

Other sciences have been working at the petabyte scale for some time.
[I don't really know the background here, it'll require some research.
Astronomics, geosciences etc have all been working with such large
datasets, and been using Python to do so, successfullly. Concrete
examples]

While some aspects of genomics data are unique and require specialised
tooling, by the time that variants have been called and the we wish
to work on the final stages of analysis, we are usually
working with large n-dimensional arrays of numerical data. Such
datasets are precisely what a large and active community of
developers have been working on in the PyData Ecosystem. The goal
of sgkit is to bring these tools to genomics, using a
shared platform for analysing large arrays of numerical data. In the
following subsections we discuss the key technologies, and how
they apply to genomics.

\subsubsection{Array oriented computing}

NumPy~\citep{harris2020array} has been transformational, and provides
the basic framework for scientific computing in Python.

[ Things to cite: pandas~\citep{mckinney2010data},
BioNumpy~\citep{rand2022bionumpy}, OME-Zarr~\citep{moore2023ome}]

[NOTE: we may also want a section discussing the n-dimensional array model,
but I think this section is the right place to discuss that?]

\subsubsection{Columnar binary data}

Perhaps the most important factor limiting scalability in contemporary genomics
is the continued reliance on row-wise storage of data---most often
encoded as blockwise-compressed text.
[Explain why row-wise is bad without reference to specific genomics applications.]

The Variant Call Format (VCF) is the standard method for interchanging
genomics data~\citep{danecek2011variant}.
In VCF each row describes the observations for a set of samples
at a given position along the genome, and consists of fixed columns such as the
genome position as well as per-sample columns. [More info]
VCF was introduced as part of the 1000 Genomes project, and works reasonably
well at the scale of 1000s of samples. However, when we have hundreds of
thousands of samples it becomes extremely unwieldy
. For example, the
phased genotypes for 200K samples in UK Biobank produced by Browning [cite]
require X GB of space.
(Note that this file contains no extra INFO keys; in practice a great deal
more information is stored in the VCF leading to files that are orders of
magnitude larger, and requiring them to be split into chunks, further
increasing their unwieldiness.)
Running [a simple query that requires only looking at
one column] on this file like X required Y minutes.

[TODO be more concrete here; what is the thing we're summarising?]
This slow response time to a simple query is primarily caused by the row-wise
nature of VCF (and BCF), because in order to read a single piece of
information about a particular variant site we must read \emph{all}
the information
about that site. Then, if we wish to summarise that information over
all sites, then we must read the entire dataset.

A lesser but still significant cause of inefficiency is the use of
text to store the data. [More info]

These issues are of course well known, and there are numerous projects
that seek to alleviate them. [Discuss] They're all quite bespoke to genomics data
through.

In sgkit we use Zarr which although initially introduced to store
genomics data, has now been adopted widely in [X, Y and Z].
Zarr is [description] and has [advantages]

Also to discuss (not sure how these fit in to this specific sectio
on background technologies vs existing genomics infrastructure; can
sort out later):

\begin{itemize}
\item columnar (with small records) good for I/O elimination + sequential access.
Classical locality of reference stuff as well.
\item What does this mean with ND arrays? Slippery concept. Discuss.
\item Compression improvements from columnar.
\item Poor performance of text-based VCF has been noted many times,
and lots of prior work in this space e.g. \citep{kelleher2013processing}
\item Honourable mentions for bcftools~\citep{danecek2021twelve},
htslib~\citep{bonfield2021htslib}, CRAM~\citep{bonfield2014scramble,bonfield2022cram}
which uses a columnar approach
\end{itemize}

\subsubsection{Distributed computing}

Beyond a certain scale of data there is no alternative but to
parallelise calculations across multiple computers if you wish
them to complete in a reasonable timeframe. Classically in
High Performance Computing (HPC) environments, most often used
to perform scientific computing, such distributed computing
is done using the Message Passing Interface (MPI). MPI
typically assumes a low-latency connection between nodes in
cluster, often with specialised hardware interconnects,
and can be used very effectively to [do things like solve big PDEs].
However, MPI is [hard to program and hard to run], and applications
in genomics have been limited to [things like the ABySS assembler].

[Paragraph describing, Big data, Hadoop, MapReduce, Spark, etc. Why
they don't use MPI, that the problem being solved was/is.
Applications to genomics like ADAM were promising, but never really
took off. Hail (see also genomics section) notably does a
bunch of things well, but only by rewriting whole chunks of the
underlying technologies]

We use Dask [for reasons].

\subsubsection{Just-in-time compilation}

Many of the analyses that we wish to perform in genomics are unique,
and the ability to quickly put together complex queries using
high-level tools like shell pipelines and scripting languages
is often a major bonus. However, such approaches are often not
viable at the petabyte scale, where efficient compiled code that
uses available CPU resources effectively are required. However,
writing code in compiled languages such as C, C++ or Java
is far more labour intensive, as well as requiring more specialised
software development experience.

Similarly, [scripting languages such as python are great for
contributing to open source libraries, because there's a much
lower barrier to contribution.]

We use numba because [it's awesome].

\subsubsection{Heterogenous computing}

GPUs are cool, we support them.

\section{Discussion}

Things to discuss:

\begin{itemize}

\item While API based approaches such as htsget~\citep{kelleher2019htsget}
and more generally federated access to data~\citep{rehm2021ga4gh}
are an important part of the future of genomics, we must still pay
attention to how efficient \emph{local} compute is. We need to
store and exchange data in an efficient and easily accessible
format. [Could talk about
how early GA4GH APIs failed because they didn't really consider
efficiency? Converting VCF to JSON was never going to be efficient?
Might not be worth getting into]

\end{itemize}


\section{Acknowledgments}

\bibliography{paper.bib}

% \subsection{Level 2 Heading}

% \subsubsection{Level 3 Heading}

% \paragraph{Level 4 Heading}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Example Table:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{table}[bt]
% \caption{\label{tab:example}Automobile Land Speed Records (GR 5-10).}
% % Use "S" column identifier to align on decimal point
% \begin{tabular}{S l l l r}
% \toprule
% {Speed (mph)} & Driver          & Car                        & Engine    & Date     \\
% \midrule
% 407.447     & Craig Breedlove & Spirit of America          & GE J47    & 8/5/63   \\
% 413.199     & Tom Green       & Wingfoot Express           & WE J46    & 10/2/64  \\
% 434.22      & Art Arfons      & Green Monster              & GE J79    & 10/5/64  \\
% 468.719     & Craig Breedlove & Spirit of America          & GE J79    & 10/13/64 \\
% 526.277     & Craig Breedlove & Spirit of America          & GE J79    & 10/15/65 \\
% 536.712     & Art Arfons      & Green Monster              & GE J79    & 10/27/65 \\
% 555.127     & Craig Breedlove & Spirit of America, Sonic 1 & GE J79    & 11/2/65  \\
% 576.553     & Art Arfons      & Green Monster              & GE J79    & 11/7/65  \\
% 600.601     & Craig Breedlove & Spirit of America, Sonic 1 & GE J79    & 11/15/65 \\
% 622.407     & Gary Gabelich   & Blue Flame                 & Rocket    & 10/23/70 \\
% 633.468     & Richard Noble   & Thrust 2                   & RR RG 146 & 10/4/83  \\
% 763.035     & Andy Green      & Thrust SSC                 & RR Spey   & 10/15/97\\
% \bottomrule
% \end{tabular}

% \medskip
% Source: \url{https://www.sedl.org/afterschool/toolkits/science/pdf/ast_sci_data_tables_sample.pdf}

% \tabledata{This is a description of a data source.}\label{tabdata:first}
% \tablesrccode{This is a description of a source code.}\label{tabsrccode:first}

% \end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% feature box
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \begin{featurebox}
% \caption{This is an example feature box}
% \label{box:simple}
% This is a feature box. It floats!
% \medskip

% \includegraphics[width=5cm]{example-image}
% \featurefig{`Figure' and `table' captions in feature boxes should be entered with \texttt{\textbackslash featurefig} and \texttt{\textbackslash featuretable}. They're not really floats.}

% \lipsum[1]
% \end{featurebox}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Figures
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \begin{figure}
% \includegraphics[width=\linewidth]{elife-13214-fig7}
% \caption{A text-width example.}
% \label{fig:view}
% %% If the optional argument in the square brackets is "none", then the caption *will not appear in the main figure at all* and only the full caption will appear under the supplementary figure at the end of the manuscript.
% %
% \figsupp[Shorter caption for main text.]
% {This is a supplementary figure's full caption, which will be used at the end of the manuscript.
%   \figsuppdata{A data source; see \url{https://doi.org/xxx}}
%   \figsuppdata{Another data source.}
%   \figsuppsrccode{And the source code.}}
% {\includegraphics[width=6cm]{frog}}\label{figsupp:sf1}
% %
% %
% \figsupp{This is another supplementary figure.}
% {\includegraphics[width=6cm]{frog}}
% %
% %
% \videosupp{This is a description of a video supplement.}\label{videosupp:sv1}
% \figdata{This is a description of a data source.}\label{figdata:first}
% \figdata{This is another description of a data source.}\label{figdata:second}
% \figsrccode{This is a description of a source code.}\label{figsrccode:first}
% \end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% APPENDICES
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \appendix
% \begin{appendixbox}
% \label{first:app}
% \section{Firstly}
% \lipsum[1]

%% Sadly, we can't use floats in the appendix boxes. So they don't "float", but use \captionof{figure}{...} and \captionof{table}{...} to get them properly caption.
% \begin{center}
% \includegraphics[width=\linewidth,height=7cm]{frog}
% \captionof{figure}{This is a figure in the appendix}
% \end{center}

% \section{Secondly}

% \lipsum[5-8]

% \begin{center}
% \includegraphics[width=\linewidth,height=7cm]{frog}
% \captionof{figure}{This is a figure in the appendix}
% \end{center}

% \end{appendixbox}

% \begin{appendixbox}
% \includegraphics[width=\linewidth,height=7cm]{frog}
% \captionof{figure}{This is a figure in the appendix}
% \end{appendixbox}

\end{document}
